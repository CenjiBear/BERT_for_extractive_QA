# BERT_for_extractive_QA_Math_Project_1
Depository for Math Winter 2023 group project


Team Members:
Sean Tran - 101449600
Mohammed Mujtaba Rabbani - 101387404


The Hugging Face models are powerful tools for extractive Q&A tasks and can handle straightforward questions and lengthy texts with ease. However, they can still produce incorrect answers with high confidence, which suggests that they may require further fine-tuning or modifications to improve their accuracy.

The customized BERT model that we fine-tuned using a semi-custom dataset was able to outperform the Hugging Face models in terms of accuracy, especially when dealing with questions that required information from longer texts. This suggests that fine-tuning a pre-trained model using a customized dataset can significantly improve its performance.

However, our customized BERT model received low probability scores, which could be due to the limited size of our dataset or the short training time. Therefore, to improve the performance of the customized model, we may need to increase the size of our dataset or increase the training time.

Overall, our evaluation highlights the importance of carefully selecting and fine-tuning models for specific Q&A tasks. While pre-trained models can be effective, they may not always be suitable for every scenario, and customized models may be necessary to achieve optimal performance.

# NLG_With_GPT_Math_Project_2:

Natural Language Generation (NLG) with GPT, This project focuses on exploring the application side of GPT (Generative Pre-trained Transformer) and its capabilities as a text generator. We delve into the inner workings of GPT, including masked attention and hyperparameter tuning, to improve the quality of generated text. Additionally, we discuss the issue of bias in GPT and the importance of being aware of it when using GPT for downstream tasks.

In this project, we also explore the concept of few-shot learning, where GPT can learn to perform new tasks with just a few examples. Our findings suggest that GPT is a powerful tool for natural language generation and has applications in various industries, including marketing, customer service, and content creation.

Prompt engineering is another powerful tool that we explore in this project to further enhance the capabilities of GPT. By providing specific prompts to GPT, we can control the output and generate text that is relevant to a specific topic or task. For example, a reader can just type in questions and see the responses generated by GPT. Prompt engineering has the potential to revolutionize the way we use natural language generation in various industries.

This project provides insights into how GPT can be used to improve text generation and generate high-quality content.
